# Chapter 7：事务

存储系统过程中可能由于多种原因导致数据存取失败，例如：

- 数据库软件或是存储硬件异常或是bug
- 使用存储系统的应用由于各种原因crash了
- 由于网络原因导致的应用无法访问数据库
- 应用可能读取到由于数据修改/写入未完全完成的无意义的数据
- 多个客户端同时读取数据导致的竞争

存储系统需要处理以上的各类错误，并且保障错误不会导致系统的雪崩宕机。事务是数据库为了解决类似的问题而出现的一个机制。它可以将多个读写操作合并为一个操作，最终递交数据库统一更新（commit），或者整体回滚，取消操作（rollback，abort）。

事务本身是制造出来的概念，目的是简化应用的编程模型（Simplify the programming model）。通过事务，应用可以忽略某些潜在的错误场景以及并发访问数据库产生的问题，因为数据库通过事务处理了这些问题。

当然，并不是任何场景都需要事务，事务的使用本身也会带来负担。通常弱隔离级别的事务机制技能满足大部分应用的场景，并且也能获得较高的性能。

## The Slippery Concept of a Transaction

几乎所有的现代关系型数据Oracle，SQL Server, PostgresQL, MySQL都支持事务，他们都遵从最早的SQL数据库，IBM System R的形式，虽然他们的实现方式可能并不一致。

目前NoSQL越来越流行，不同于关系型数据库，他们提供了新的数据模型的选择，同时也提供了复制，数据分片能力。关系型数据库需要支持的事务也是NoSQL开始发展的一个原因，几乎所有的新的NoSQL都抛弃了事务，或则提供了一个相对于事务更为脆弱的机制。

有观点认为，事务与扩展性是矛盾的，任何大型的存储系统，应该掘弃对于事务的支持，以获得更高的可用性和性能。事务功能有时被数据库供应商作为管理高价值数据的应用所必须的功能。以上的两种观点都有点夸大了。

与其他技术设计选型类似，事务由它的优势，也有它的局限性。为了了解这些trade-off，我们需要先了解数据库所能够提供保障。

### The Meaning Of ACID

事务能够提供的安全机制，最常被描述为ACID（原子性atomicity，一致性consistency，隔离性isolation，持久性Duyrability）。

数据库实现的ACID之前是不同，最为常见的是isolation，不同的数据库实现了不同的隔离级别。因为当一个系统声称其实现了ACID，我们也不能确认它所能够提供数据安全保障。

#### 原子性

通常，原子性表示是不可切分的。在多线程编程中，一个线程执行一个原子性的操作，其他线程是不能看到线程执行到一半的结果的。

**数据库的原子性与并发不同，它描述的是，如果某一个客户端执行的事务会进行多个写入，这个过程中如果出现各种错误或是回滚，例如客户端与服务端的网络中断，写入的磁盘满了，写入的应用进程crash了，或者写入的数据会破坏唯一索引等一致性约束，客户端中断了事务等，原子性机制保证多个写入会被一起写入或者一个都不写入。**

#### 一致性

一致性的含义已经超负荷了。

1. 在Chapter 5中，我们讨论了**副本一致性**（replica consistency）以及异步复制系统的**最终一致性**（eventual consistency）导致的问题
2. 在分区章节中，我们讨论了**一致性哈希（Consist Hashing）**作为数据进行分区的方案。
3. 在CAP理论（Chapter 9），字母C所代表的一致性，实际上指的是**线性一致性**（linerizability）
4. 在ACID的上下文中，一致性表示是应用层认为数据库的数据处于一个正常状态。

在不同的场景下，一致性所表达的含义是不同的。

ACID的一致性保证的是应用层认为的数据的一致。例如在一个账户系统中，存入金额和账户余额应当保持数据一致。如果事务过程中，应用层写入的数据是校验正确的，那数据库应当保证存储的数据也是正确的。

**原子性，隔离性，以及持久性是数据库的属性，而一致性更多的是一个应用的属性。应用通过数据库的原子性和隔离性保证数据的一致性，而不是通过数据库自身保证一致性。**Consitency实际上不应当包括在数据库的ACID机制内。

#### 隔离性

多个客户端可以同时访问数据库，如果他们访问/修改的同样的数据，则可能导致并发问题。

![1554016613502](http://liang2020.oss-cn-hangzhou.aliyuncs.com/uPic/blog/1554016613502.png)

以上的场景中，User1与User2同时访问并更新某一行数据，实际上的结果应该是42 + 1 + 1 = 44，但是由于并发问题，导致最终的结果为43。

隔离性指的是，不同的事务之间是相互隔离的，他们不会相互影响。可串行化是最高级别的隔离，它表示，每一个事务都是数据库中运行的唯一事务，从而避免并发问题。

由于性能问题，可串行化的隔离级别很少被使用，通常数据库会使用可重复读，提交读等作为生产环境的事务隔离配置。在Oracle 11g上，甚而没有实现串行化的隔离级别，他们通过*snapshot isolation*实现一种较弱的序列化。

MySQL可以设置的隔离级别包括了：

- READ UNCOMMITTED(未提交读)
  - 事务能够读取到其他事务未提交的数据
- READ COMMIT(提交读)
  - 又称不可重复读（nonrepeatable read），只能读取到已经被其他事务提交的数据。无法避免脏读的问题，执行两次相同的查询可能获得不同的结果。
- REPEATABLE READ(可重复读)
  - 保证了同一个事务多次读取同样记录获得的结果是相同的，Innodb，XtraDB引擎通过多版本控制（MVCC，Multiversion Concurrency Control）实现，不过无法避免幻读的问题。
  - 幻读（Phantom Read）指的是，事务A执行过程中读取了某一个范围的数据，另外的事务B又在改范围内插入新的记录，事务A重新读取时，则会产生幻行（Phantom Row）
- SERIALIZABLE(可串行化)
  - 可串行化是MySQL最高的隔离级别，通过串行化执行事务，避免了并发问题。不过由于它的实现会有巨大的性能成本，因而极少在生产环境中使用。

**MySQL的不同的隔离级别**

| 隔离级别 | 脏读可能性 | 不可重复读可能性 | 幻读可能性 |
| -------- | ---------- | ---------------- | ---------- |
| 未提交读 | ✔          | ✔                | ✔          |
| 提交读   | -          | ✔                | ✔          |
| 可重复读 | -          | -                | ✔          |
| 串行化   | -          | -                | -          |

#### Duration

持久性，指的是事务提交后，数据会被持久存储，不会丢失。在单机数据库中，持久性表示数据被持久化到磁盘。在可复制数据库中，持久性表示数据被复制到一定数目的节点。实际上，持久化是无法保障数据不会丢失的，磁盘可能损坏，异步复制系统中，可能由于网络原因，导致leader宕机等。



## Weak Isolation Levels

并发会带来复杂度，数据库通过事务的机制，对应用层隐藏数据的并发访问，从而降低应用层开发的复杂度。串行化的隔离级别能够杜绝各种并发问题，不过它会严重得拖累数据库的性能。通常生产实践上，会使用更弱的隔离级别，例如提交读，快读隔离/可重复读等，不过他们并不能完全避免并发问题。

### 提交读（Read Committed，RC）

提交读能够避免脏读（dirty read）和脏写（dirty write）的问题。

> RC隔离级别下，事务只会读取到已经被commit的写入（no dirty read），并且只会覆盖已经被commit的数据（no dirty write）

- 提交读的实现

通过锁阻止对于数据的并发访问和修改，在事务提交后自动释放，从而实现提交读机制。

不过如果通过锁阻止其他事务对于已经写入，但还未提交的数据的读取，对于其他事务的性能会造成严重影响，并且会降低系统的吞吐。

数据库一般使用保存已提交的数据和未提交的数据多份拷贝实现。未提交的数据通过锁进行保护，而读行为只会读取到已经提交的数据拷贝，锁并不会阻塞数据读取。

### 可重复读（Repeatable Read）和快照隔离（Snapshot Isolation）

RC场景下，可能导致读偏斜（Read Skew）出现，考虑以下场景

![1555239204056](http://liang2020.oss-cn-hangzhou.aliyuncs.com/uPic/blog/1555239204056.png)



由于Alice读取数据的过程中，与Transfer流程并发，导致了，读取的Account 1的金额是转账前500，而Account2的余额是转账后的400，总额变成了900，而实际的金额总额应该是1000。

读偏斜在RC级别下被认为是可接受的，因为Alice读取账号2时，账号2的余额信息已经被修改了。如果Alice重新访问一次，她会发现余额总额是正确的。

不过并不是所有的场景都能接受读偏斜的出现：

- 数据备份：如果数据备份过程中出现了读偏斜，那么这个备份的数据的一致已经已经被破坏。
- 完整性校验：如果需要对数据库中的数据，做完整性校验，但出现read skew时，数据库的完整校验就失败了。

快照隔离级别能够避免Read Skew的问题。它的基本思想是，事务能够访问到它开始时整个数据库中已经提交的数据的快照.及时这些数据在事务执行过程中被其他的事务提交修改了，当前事务依然只会读取它开始时的那个快照.

 现代主流的关系型数据库普遍都支持快照隔离, 包括Oracle, PostgreSQL, 使用InnoDB引擎的MySQL, SQL Server.

#### 快照隔离的实现

与Read Committed隔离级别实现类似, 快照隔离使用write lock阻止dirty write, 它意味着, 事务通过锁保证不会有其他事务同时写入某一条数据。 从性能角度考，快照隔离的实现的原则是，*读操作不会阻塞写操作，写操作不会阻塞读操作*。

现代关系型数据库的快照隔离一般使用多版本并发控制机制（Multi-Version Concurrency Control）实现。Read Committed实现中，保留两个版本的数据，已提交的和未提交的。它是MVCC机制的一个简化版本，MVCC机制会保留多个Committed的数据版本。参考下图

![1555244162791](http://liang2020.oss-cn-hangzhou.aliyuncs.com/uPic/blog/1555244162791.png)

MVCC机制中，每一个事务都有一个唯一的事务id（txid），每一份数据拷贝，除了原本定义表结构数据字段，还有两个字段，包括created by该版本数据由哪个字段创建，deleted by该版本数据由哪一个事务修改覆盖了。上图中的流程，如果使用Read Committed机制，则会导致txid=12的事务在第二次读取account 2时读取到的数据字段未已经提交的结果，balance=400，导致读偏斜。使用快照隔离后，第二次读取会选择事务开始时的数据快照，balance=500，从而避免了读偏斜。

#### 快照隔离的一致性快照规则

每一个事务都有一个唯一的事务ID，快照隔离的事务能够可见的一致性数据快照主要通过以下规则判断。

- 当事务开始时，那些还未提交的事务的写入会被忽略，及时那些事务在当前事务结束前就提交了。
- 忽略所有退出的事务的快照数据
- 忽略所有事务ID大于当前事务的ID的数据版本
- 所有的其他事务的数据版本对于当前事务可见

上图的案例中，由于事务13晚于事务12，因而它的写入对于事务12是不可见的。

另外一种方式理解，只有符合以下规则的数据对象对于事务是可见的。

- 在当前事务开始之前，被创建并且已经提交的对象
- 对象还未被删除，或者删除操作在当前事务开始之前还未被提交



> **可重复读以及命名混淆**
>
> 快照隔离是一个非常使用的隔离级别，不过在不同的关系型数据库中，他们的命名时不同的。在Oracle中，被成为serializable 隔离级别，而在PostgreSQL以及MySQL中被称为Repeatable Read，可重复读隔离级别。
>
>  究其原因，是由于SQL标准并没有对于快照隔离的定义。SQL标准基于IBM SystemR，在1975年诞生，而当时快照隔离还未诞生。



### 避免更新丢失（Prevent Lost Update）

快照隔离能够避免dirty write的问题，不过并发写入还有可能导致write-write conflict。**写入丢失**（lost update）是比较常见的write-write conflict。

写入丢失问题描述的是多个事务，采用读取数据，根据读取的数据修改获得新的数据，将新的数据写回数据库中，如果有多个写入事务并发操作，可能导致先写入事务的数据被后写入事务的数据覆盖的问题。

类似的写入丢失问题可以在许多场景下发生，例如：

- 通过读取计数器，修改计数器，写回数据库。或则通过读取余额，修改余额写回数据库
- 多个事务同时并发修改一个复杂对象，例如一个JSON对象，由于需要将完整的对象写回，不同事务修改了复杂对象的不同字段属性，导致了写入丢失

以下的这些策略可以避免更新丢失



#### 使用原子性写入

考虑以下的SQL

```sql
UPDATE counters SET value = value + 1 WHERE key = 'foo';
```

通过类似原子性操作的方式，可以避免读取，修改，写入中间出现其他并发操作，导致数据更新丢失。

不过并不是所有的操作都适合修改为原子性操作，例如对于复杂对象的修改，例如需要某些数据操作需要在应用层才能知道数据修改的结果。

#### 使用显示锁

可以通过锁阻止其他的事务并发进行读取-修改-写入操作，考虑以下SQL

```sql
BEGIN TRANSACTION;
SELECT * FROM figures
WHERE name = 'robot' AND game_id = 222
FOR UPDATE;
-- Check whether move is valid, then update the position
-- of the piece that was returned by the previous SELECT.
UPDATE figures SET position = 'c4' WHERE id = 1234;
COMMIT;
```

通过锁可以保证事务对于某一个数据的修改会串行化执行。

#### 自动检测更新丢失

原子性操作以及显示锁能够强制让可能导致更新丢失的操作串行化执行。另外一种替代方案是，数据库允许并发的并发执行，当检测到如果有可能导致更新丢失的事务，则直接退出事务，让其重试。

Oracle，PostgreSQL，SQL Server的快照隔离级别都支持类似的机制，能够自动检测导致更新丢失的事务。MySQL并不支持类似的特性。类似的特性对于应用层更为友好，应用层无需无需担心由于忘记加锁导致的并发写入丢失问题，数据库会完成并发丢失问题的检测和处理。

#### 比较并交换 Compare and Set

考虑以下SQL

```sql
UPDATE wiki_pages SET content = 'new content'
WHERE id = 1234 AND content = 'old content';
```

通过Compare And Set 操作，我们可以实现原子性得修改数据库字段。不过如果以上的SQL能够读取到老的版本，例如在快照隔离界别中，它会读取到老的一致性快照数据，它的compare操作比较的是老的副本，是没有意义的。



### 写偏移与幻读

以上的章节中，我们讨论了如何避免dirty write以及lost update，主要是由于多个事务对于同一个对象的并发写入导致的问题。除此之外，并发还有导致其他的问题。

考虑以下场景，医院要求至少要有一名医生晚上值班在岗，Alice和Bob目前同时在岗，他们发起了查询，确认当前在岗医生数量，已确认自己是否可以离岗。

![1555250663609](http://liang2020.oss-cn-hangzhou.aliyuncs.com/uPic/blog/1555250663609.png)

以上的两个事务并发执行，由于Alice和Bob都看到，当前有两名医生在岗，因而他们都认为自己可以离岗，因而导致了最终没有医生在岗。

#### 写偏移（Write Skew）

类似的问题被称为写偏移，与dirty write和lost update不同，它不是由于并发写入同个对象导致的。它同样是由并发导致的，试想，如果这两个事务串行化执行，则不会出现类似的问题。

可以将写偏移作为写入丢失问题的泛化表现，它首先是不同的事务是读取了一些数据，然后根据读取的结果更新了数据库的某些数据。如果事务更新是相同的数据，则是lost update和dirty write问题。



#### 写偏移的其他例子

- 房间预定：原本约定，一个房间在同一个时间，只能被一个人预定。考虑以下SQL

```sql
BEGIN TRANSACTION;
-- Check for any existing bookings that overlap with the period of noon-1pm
SELECT COUNT(*) FROM bookings
WHERE room_id = 123 AND
end_time > '2015-01-01 12:00' AND start_time < '2015-01-01 13:00';
-- If the previous query returned zero:
INSERT INTO bookings
(room_id, start_time, end_time, user_id)
VALUES (123, '2015-01-01 12:00', '2015-01-01 13:00', 666);
COMMIT;
```

在快照隔离级别下，由于select读取的是快照，如果由并发的事务执行，则可能出现同一时间多个人预定房间。



- 用户起名：一个网站，约定了一个名字如果被某个用户使用了，则不能被其他用户使用。如果并发的SQL先查名字是否已经被使用，再起名，则可能导致并发冲突，同个名字被多人使用。



#### 幻读导致了写偏移

类似的案例都符合以下的流程：

- 一个SELECT语句搜索需要的条件
- 应用层根据SELECT的结果如何继续执行
- 如果应用层确认继续，则执行一个DML（SELECT，UPDATE，DELETE）操作。

当然，以上的步骤的顺序并不是确定的，例如可能先插入数据，然后SELECT操作，确认数据是否符合要求。

在医生值班的案例中，我们可以通过锁机制（SELECT ... FOR UPDATE），避免写偏移的出现。不过在其他案例里，因而后续的是insert操作，SELECT的数据还不存在，我们无法通过锁完成。

类似的现象，是一个事务的写入操作，改变了另一个事务的SELECT查询的结果，被称为幻读。快照隔离无法解决幻读导致的写偏移问题。

## Serializability串行化

串行化隔离级别(Serializable Isolation)是最强的隔离级别，它能够让事务如同逐一运行一样，不会互相干扰。它能够避免Weak Isolation隔离级别在并发场景下遇到的各类问题，不过通常它们都不是第一选择。现在的数据库系统，主要通过以下三种方式之一实现串行化隔离级别。

- 按照一定的顺序，逐一运行事务
- 通过两阶段锁（Two-phase Locking，2PL）实现
- 优化的并发控制：可串行化快照隔离（Serializable Snapshot Isolation）

### 真正的串行化执行

最简单的实现串行化隔离级别的方法是移出所有的并发冲突：数据库在同一时间只会执行一个事务，所有的事务按顺序，在一个线程中逐一执行。在过去数十年中，它的性能被认为是十分低效的，不过这个认知正在改变：

- 内存变得越来越廉价，这导致数据库可以将所有的数据集放在内存中，对于数据的主要操作都在内存中进行，事务的执行速度可以非常迅速。
- 绝大多数的OLTP（Online Transaction Process）通常是由多个简单的读或是写组成，通常不会长时间运行。

Redis, Datomic, VoltDB/H-Store通过串行化执行实现串行化隔离级别，由于避免了多线程下的锁协调的成本，它们在部分场景下，甚而更加高效。不过由于是单线程执行，无法利用CPU的多核的能力，CPU非常容易称为数据库的一个瓶颈。

#### 通过存储过程包裹事务

许多应用场景下，可能会出现多个并发同时运行的事务，例如用户在飞机上选座过程中，会有多个文本输入的流程，座位选择的流程，我们无法控制应用层输入的进度，类似的场景下，会有多个同时活跃的事务，单线程的串行化执行是无法适用于类似的场景。

**存储过程**（stored procedure）可以解决类似的问题，它通过将整个事务需要执行的代码一起提交，避免了多个事务同时运行的需求。

以下的案例中，应用层需要根据当前有多少个医生，觉得某一个医生是否能够离岗。交互过程的事务中，应用层会查询当前有多少个医生在岗，根据查询结果，进行下一步操作，这个过程中应用层会与数据库有多次通讯。而通过存储过程实现中，提交的事务会在一次提交中执行。

![1557660783619](http://liang2020.oss-cn-hangzhou.aliyuncs.com/uPic/blog/1557660783619.png)

#### 存储过程的优缺点

存储过程是SQL标准之一，它存在一些缺陷：

- 每一个关系型数据库的存储过程使用的语言各不相同（Oracle PL/SQL， SQLServer T-SQL， Post'g'reSQL PL/pgSQL)，这些语言无法满足一般的开发需求，缺少完善的库。
- 在数据库中运行的代码难于管理，无法使用版本管理工具，难于部署，难于测试，不能与监控系统良好结合。
- 数据库对于性能比较敏感，一个实例通常需要服务于多个应用服务，一个性能较差的存储过程会导致数据库性能被拖累。

以上的这些缺陷并不在所有的数据库中都是如此。许多现代数据库都抛弃了使用SQL实现存储过程：VoltDB使用Java或Groovy，Datomic使用Java或Clojure，而Redis使用lua。

存储过程以及in-memory数据，使得在单线程中执行所有事务变得更加可行。它们不需要等待I/O，并且消除了并发控制带来的性能损耗，在单线程环境下也能获得比较高的性能。

VoltDB使用存储过程实现复制：它通过在不同的副本上执行相同的存储过程，替代了将事务的写入从节点复制到另一个节点。不过这也要求了VoltDB的存储过程是**确定性的（deterministic，在不同的节点上的执行结果是一致的）**

#### 分区

单线程串行化执行事务，导致CPU成为一个严重的瓶颈。为了实现使用多核，甚而多个服务器节点的能力，需要对数据进行分区。VoltDB使用类似的方式实现扩展。如果数据分区能够实现每一个事务只关注当前分区的数据，那数据库的性能会随着分区数量上升线性提高。

不过如果需要实现跨分区的事务，则需要引入锁机制进行协调，已确保所有事务的串行化执行。跨分区的事务会导致数据库的性能显著下降，VoltDB的数据显示，跨分区事务下，性能下降到1000 tps，并且会随着机器的增加继续下降。

事务是否能够在一个分区内执行非常依赖于数据的结构。Key Value形式的数据容易被分区，但是如果有类似二级索引的数据结构，则需要有一系列的跨分区协调机制。

#### 串行化执行的总结

串行化执行实现串行化隔离级别需要一些约束条件。

- 每一个事务必须足够小，并且快速执行，任何一个慢事务会导致后续所有的事务执行被阻塞。
- 热数据集需要能直接加载在内存中，冷数据集可以通过访问磁盘加载。
- 读写吞吐足够低，能够通过一个单核CPU处理。或则事务可以分区处理，并且不会有跨分区的事务。
- 如果有需要跨分区事务的需求，需要限制这些事务的吞吐量。

### 两阶段锁（Two—Phase Locking，2PL）

2PL是过去30年中，唯一被数据库实现串行化使用的算法。在之前的章节中，我们使用锁机制防止dirty write：如果多个事务需要并发写入相同的对象，可以通过锁机制确保一个事务需要在另一个事务完成事务流程后才能继续。

2PL的机制使用类似的机制，不过它使用更强的锁机制。

- 如果事务A读取了一个对象，事务B对对应的对象进行写入，事务B需要等待到事务提交，回滚，或则退出。
- 如果事务A写入了一个对象，事务B希望能够读取对应的对象，事务B需要等待A完成提交或则退出事务才能继续。

**在2PL中，writer不单单会阻塞writer，也会阻塞reader。并且，reader也会同样阻塞writer。而在快照隔离级别中，reader不会阻塞writer，而writer不会阻塞reader。**由于2PL能够确保串行化执行，它能够避免之前提到的各类竞性问题，包括更新丢失，以及write skew。

#### 2PL的实现

MySQL（InnoDB）和SQL Server通过2PL实现串行化隔离界别，DB2使用2PL实现可重复读隔离级别。对于reader和writer的阻塞，使用过锁实现的。这些锁可以是共享锁或是排他锁。

- 如果事务需要读取一个对象，会首先获得一个共享锁。多个事务可以获得同一个对象的共享锁，不过如果对象被一个排他锁持有，则不能获得共享锁。
- 如果事务需要写入一个对象，首先需要获得对象的排他锁。其他事务需要获得对应对象的排他锁或是共享锁，需要等待对象的排他锁释放。
- 如果事务先是读取了一个对象，然后进行写入，它需要将对对应对象的锁由共享锁升级或排他锁，过程如同获得一个新的排他锁。
- 事务获得一个锁之后，会持续持有锁直到事务结束。**这也是2PL的两阶段名称的来源：第一阶段是事务执行期间，锁被获得。第二阶段是事务结束，所有的锁被释放**，**两个阶段不相交**。

![1558269084181](http://liang2020.oss-cn-hangzhou.aliyuncs.com/uPic/blog/M38Li4unFstec5b.png)

由于大量的锁被使用，非常容易出现死锁问题。数据库会检测死锁，并且停止某一个事务。退出的事务需要应用层发起重试。

#### 2PL的性能

2PL比较早就被提出，由于性能问题，在生产实践中使用较少。

- 并发事务容易出现由于等待其他事务释放锁而等待，这导致数据库的执行事务的性能取决于事务冲突，高百分位数的延迟较高。
- 如同锁实现的Read Committed，2PL实现容易出现死锁问题，并且更加频繁，从而导致事务回滚，需要重新执行，导致数据库负载加重。
- 锁带来的成本开销(这块反而是其次的)。

#### 谓词锁（Predict Lock）

幻读，指的是在一个事务中读取的数据被其他并发事务修改，串行化隔离级别能够预防幻读问题。2PL实现的串行化隔离级别中，通过谓词锁Predict Lock避免数据被其他事务修改。

举个例子，如果一个事务需要首先查询一个房间在某个时段是否被预定(query 1)，在进行数据插入(query 2)。在2PL实现的串行化隔离级别中，query 1会使用谓词锁对符合搜索条件的记录加锁（包括还不存在的数据）。

```sql
--- query 1
SELECT * FROM bookings 
	WHERE room_id = 123 AND 
		end_time > '2018-01-01 12:00' AND
		start_time < '2018-01-01 13:00';
```

谓词锁通过以下方式工作：

- 如果事务A想要读取（SELECT）符合要求的数据，则需要获得这些条件对应记录的共享锁。如果事务B持有的排他锁的记录和事务A期望获得锁的记录重叠，则事务A需要等待事务B完成，释放锁。
- 如果事务A需要修改数据（UPDATE，DELETE，INSERT），他首先需要确认新的值以及老的值是否符合某一个谓词锁。如果存在，它需要等待对应谓词锁的事务结束。

#### 区间索引锁(Index-Ranged Lock)

谓词锁的性能不佳：如果同时存在多个并发事务，他们各自持有自己的谓词锁，查询记录是否符合某一个锁的成本及其费时。大多数使用2PL的数据库，使用索引区间锁（Index-ranged Lock，又称next key locking）作为替代。

索引区间锁是谓词锁的简化版本，通过查询使用的索引条件，锁住一个比原先谓词锁更大的区间。如果其他事务需要修改数据，需要确认数据是否与目前的索引区间锁存在冲突。在以上的SQL案例中：

- 如果`room_id`是一个索引字段，则数据库可以在索引上添加对应索引的共享锁，表示当前有事务在查询`room_id`为123的预定。
- 如果`start_time`和`end_time`为一个索引，则会在对应的索引区间上添加一个共享锁，表示有事务正在查询对应区间的books

如果query没有合适的索引可以使用，则会退化为表锁。

### 串行化快照隔离（Serializable Snapshot Isolation ，SSI）

Serializable Snapshot Isolation算法是2008年Michael Cahill在他的博士论文中提出，它能够使得数据库在损失比较少的性能的前提下，实现事务的串行化隔离。在PostgreSQL的串行化隔离级别使用SSI实现（单节点），FoundationDB（分布式）也是使用SSI实现事务。

#### 悲观与乐观并发控制

2PL算法是一种悲观并发控制手段，它假定所有读取的数据都可能会由于被其他事务修改而不符合事务预期。**SSI则是一种乐观并发控制机制，它并不会block可能的导致数据不一致的语句，在事务提交时，如果事务能够符合串行化隔离级别要求，则会正常提交，反之则会退出事务。**

乐观并发在比较高的资源冲突场景下，例如大量的事务竞争修改相同的对象，则会导致大量的事务被退出重试，导致系统吞吐下降，负载提高。在事务的资源竞争比较稀疏的场景下，性能优于2PL。

SSI是基于快照隔离，所有的读是在数据库的一致性快照上完成。这个是SSI区别于之前的乐观并发控制技术。在快照隔离的基础上，SSI使用一个算法确认写是否导致事务冲突，确认事务是否应该退出。

#### 依赖于过期数据的决定

在之前的章节中，我们讨论了write skew（写偏斜）。事务读取数据，依赖于这些数据，确认后续的数据写入。在快照隔离级别下，之前读取的数据可能被其他事务修改，事务之前读取的数据在提交时，已经是过期的数据，导致最终做出了错误的决定。

应用层在读取数据时，数据库并不知晓应用层的逻辑，这些数据如何被使用。为了实现串行化隔离，数据库需要检测事务读取的数据是否过期，决定是否需要退出事务。

数据库需要考虑以下两种情况：

- **检测事务是否读取了一个过期的MVCC对象版本（在事务读取之前，其他事务的未提交写入）**
- **检测事务写入是否影响了其他事务的读（事务的写入发生在其他的事务读取之后）**

#### 检测过期MVCC读取

快照隔离通常使用多版本并发控制实现（MVCC），事务读取时，会读取数据库的一致性快照版本，并且忽视还未提交的写入数据。下图的案例中，Transaction 43读取到的on_call=true的结果集是2个，还未被提交的写入被忽略了。但Transaction 43提交时，它之前的读取结果已经被改变了，在它提交时已经变为”过期“的数据，因而它无法保证串行化隔离级别，需要退出。

![1558876020948](http://liang2020.oss-cn-hangzhou.aliyuncs.com/uPic/blog/1558876020948.png)

为了保证不读到”过期“数据，数据库需要能够追钟事务是否由于MVCC的规则忽略了其他事务的写入。当事务提交时，数据库需要检查是否存在被忽略的写入被提交了。如果是，则当前事务之前读取到的数据已经不能保证，因而需要退出。

为什么Transaction 43检测到有一个新版本的数据写入不直接退出呢。存在可能Transaction 43是只读事务，它不会修改数据，并不会有write skew风险。同时，transaction 43的写入可能由于其他的原因被abort了，Transaction 42读取的数据在commit时，不是过期的。

#### 检测影响之前读取的写入

考虑以下的案例，2PL可以通过锁机制(Index-ranged locks）阻止对于shift_id=1234的数据的并发读写。SSI使用类似的技术，不过SSI的锁并不会阻塞其他事务。

![1558877859550](http://liang2020.oss-cn-hangzhou.aliyuncs.com/uPic/blog/1558877859550.png)

Transaction 42和Transaction 43同时搜索对于shift_id=1234的on_call的医生，数据库通过shift_id=1234的索引记录transaction 42和43读取了相关记录（如同2PL，如果没有索引，则会退化到记录整张表）。当transaction 42提交时，它需要找到所有当前执行中的，读取了相关记录的事务。这个行为相当于在索引区间内（shift_id=1234）上加上一个写锁，不过它不过阻塞其他事务，而是通知其他事务，他们之前读取的数据已经被修改过期了。

#### SSI的性能表现

工程实现的细节影响算法在实践中的表现。一个trade-off的选择是，关于事务的读写的粒度的选择。如果数据库追踪每一个事务详细读写，那么对于事务是否需要退出的判断也会更加精确，但是对于这些追踪凭证也必然耗费大量的资源。在一些场景下，事务的数据读取如果被其他事务修改了，并不会导致事务的串行化隔离界别被破坏，PostgreSQL通过类似的手段优化，减少了不必要的事务退出。

与2PL相比，SSI最大的优点是事务之间不需要等待锁的获取。如同快照隔离，writer不会阻塞reader，reader也不会阻塞writer。这使得它的性能可以较为准确量化，更为稳定。特别是对于read-only事务，它可以在一致性快照上完成而不需要任何锁，因而SSI对于读多的场景更加适合。

与纯粹的串行化执行相比，SSI并不受限于单个CPU的吞吐：FoundationDB是一个分布式的SSI实现，支持多个节点之间的并发冲突检测，即使数据是分布在多个分区节点中。

事务退出的比例直接影响SSI的性能，如果一个长时间读取和写入的事务，非常容易导致冲突退出。因而SSI期望读写事务能够尽量短（FoundationDB开源版本 只支持5秒的事务）。