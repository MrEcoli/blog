# Distributed Data: Chapter 5 Replication

# 前言

数据存储的`复制`Replicaiton，指的是通过网络，在其他服务器上维护一份数据的副本。复制能够主要有以下的功能：

- 提高系统可用性：
  - 即使某一份数据所在的服务器因为宕机导致无法访问，可以通过使用其他副本数据，避免宕机带来的影响。
- 提供系统的数据读取的吞吐
  - 同样的数据，通过复制多份，可以多台服务器上同时进行访问，读取数据的吞吐也就不再局限在原始数据所在服务器的硬件设施和负载上了。

- 使数据在物理地址上接近用户，以便于用户能够更快速的访问数据
  - 上海到洛杉矶的物理距离是10460公里，即使按照光速直线传输（299792.458 千米/秒），光电信号的一次来回也至少需要70ms。

如果复制的数据不会改变，那复制是一件很easy的事情，我们只需要将数据完成一次Copy即可。然而，现实的存储系统内，数据都是不停变化的，或增或减，或是被修改。如何在数据不停变化过程中完成数据的复制呢。本章主要介绍了三种流行的复制系统：*single-leader*， *multi-leader*, *leader-less*。

​	在复制的实现上，需要做许多取舍trade-off，例如使用异步复制还是同步复制，如何处理宕机的副本等。这些选项，是一些存储系统的可配置项，本文中也会涉及这些取舍带来的好处和坏处。

## 1. Leaders And Followers

Replica副本，指的是数据的某一份拷贝，复制系统的目的，是保证每一份副本的数据是一致的。

Leader-based replication，主从复制是比较常见的复制方式：

1. 某一份副本是主副本，leader，其他是从副本，follower，客户端只能将数据写入主副本。
2. 当数据写入发生时，leader除了需要将数据写入本地，也需要将写入数据的日志，Replication log传输给其他副本，，其他副本根据复制日志完成数据的修改。
3. 客户端可以通过follower或者leader读取数据，但是只能往leader写入数据。

![image-20180916221553384](http://liang2020.oss-cn-hangzhou.aliyuncs.com/uPic/blog/image-20180916221553384.png)



Leader-based replication是许多数据系统的功能。关系型数据库：PostgreSQL，MySQL，Oracle Data Guard，SQL Server内置的数据复制功能。 非关系型数据库，例如，MongoDB，RethinkDB，Eepresso等也支持。分布式消息系统：Kafka，RabbitMQ等。网络文件系统：DRBD。

### 1.1 同步还是异步复制

考虑一下的以下这张图

1. **follower1与leader之间是同步复制**，当Leader进行数据写入时，会先生成一条复制日志，复制日志同步到Follower1，当follower1接收到复制日志，完成数据写入操作后，将写入结果通知Leader1，leader在接收到follower1写入结果后，才会完成数据的写入。同步复制指的是：**只有当follower完成数据写入，并且通知leader之后，leader才会完成数据写入。** （这里的写入指的是数据修改，可能是删除，修改，或者数据增加）
2. **follower2与leader之间是异步复制**，当Leader接到数据写入任务后，会直接完成数据的写入，将复制日志同步给follower2。**leader是否完成数据写入，并不取决于follower是否成功完成数据写入**。

![image-20180916222701620](http://liang2020.oss-cn-hangzhou.aliyuncs.com/uPic/blog/image-20180916222701620.png)



​	同步的复制，确保了数据写到follower后才完成数据的写入，确保了数据的可靠性。由于数据写入需要依赖于复制日志的网络传输，follower的数据写入，以及follower数据写入结果的响应，会带来写入性能上的影响。如果有多个follower，并且都是同步复制，leader写入的时间取决于最慢的follower的响应，一个follower服务器的异常则会导致leader写入失败或是写入时间花费过长。如果follower是跨地域分布，例如leader在杭州，某一台follower在洛杉矶，则同步复制带来的写入成本是巨大的。

​	异步复制，不会阻塞leader的写入，但是可能带来数据上的丢失。例如：如果leader在数据写入之后宕机，follower被提升为leader，而follower由于网络丢包导致复制日志丢失，则可能带来，客户端认为数据已经写入成功，然而新的leader上并没有之前leader上已经写入的数据，造成数据丢失。

​	使用半同步复制（semi-replicaiton）可以一定程度上避免同步复制的缺陷。设置一个follower为同步复制，多个leader为异步复制。当同步复制的follower出现异常时，异步复制的follower可以提升为同步复制的follower，避免由于follower异常导致的写入异常。



### 1.2 初始化新的follower

由于各类原因，我们可能需要增加新的follower，例如原先的follower宕机了，或则我们期望通过增加follower提高可用性，提高数据吞吐等。一般情况下，增加新的follower会有以下步骤

1. 根据leader创建一份数据快照
2. 将数据快照传输给follower所在服务器，follower根据快照创建新的实例
3. follower根据快照截止的复制日志，向leader请求所有快照之后的复制日志。MySQL通过binlog coordinates标识快照的复制位置。
4. follower通过执行复制日志赶上leader后，继续处理leader传输过来的复制日志。

### 1.3 处理节点异常

leader和follower都可能由于各类原因，例如磁盘损坏，地区光缆中断，断电，等各类因素导致节点数据延迟，或则不可用。我们需要在各类failure发生后，降低对于应用层的影响。

#### Follower Failure：Catched Up

每一个follower都能够记录自身执行的复制日志的位置，通过向leader请求后续的复制日志，完成丢失日志的复制，并追赶上leader。

#### Leader Failure：Failover

leader的异常的处理机制要复杂得多，一个follower需要被提升为Leader，客户端需要知道，原先的Leader不可用，需要使用新的leader进行写入，其他的follower需要知道，需要从新的leader处获取复制日志。整个过程又被称为**故障转移 Failover**。

failover的处理可以由运维手动完成，也可以由程序自动完成，主要会有以下步骤：

1. 确认Leader是否已经宕机
   - 现实世界里，可能会有多种异常发生，断电，网络异常，磁盘故障，或则其他。并没有能够十分简单的可以检测leader是否异常的方法。许多系统通过leader的响应时间判断是否异常，如果响应超时，无法连接，则判断leader异常。
2. 选择一个新的Leader
   - 新的leader一般通过选举产生，或则预先配置的候选人作为Leader。最佳候选人，通常是数据与Leader最为接近的副本。leader的选举有许多一致性算法实现，后续也会有介绍。
3. 重新配置系统，使用新的leader



Failover可能遇到的问题：

1. 异步复制可能导致数据不一致，新的leader没有接收到老的leader的部分数据，导致新的leader向follower进行发生复制日志时，follower上出现数据不一致，导致数据写入失败。
2. 数据写入丢失影响系统的可用性：GitHub曾今出现过一个事故，一个与leader数据不一致的MySQL follower被提升为Leader，follower的自增主键小于Leader，而部分Redis的数据是依赖于这个主键的，Failover发生后，导致新生成了一些之前已被使用的主键，导致Redis内的数据被覆盖了。
3. 一定场景下，例如网络异常，部分服务器之间网络不通，可能出现多个Leader，该现象又被成为脑裂。同时有两个leader在进行数据写入，将带来灾难性的事故。
4. 如何确认timeout的时间已判断leader是否真的不可用。有可能leader只是由于系统负载较高，或则网络延迟，无法及时响应，较低的timeout可能带来误判。太长的timeout则可能带来系统无法及时响应故障进行failover。



### 1.4 复制日志的实现

#### 基于语句的复制日志 

基于语句的复制(Statement-based replication)指的是，直接将客户端修改数据的请求作为日志。对于关系型数据库，这些请求指的是数据修改语句，DELETE, UPDATE, INSERT等。MySQL在5.1之前使用基于语句的复制实现主从复制，基于语句的复制存在一些缺陷：

- 执行结果非确定性的语句，可能导致不同的副本上出现数据不一致。例如使用`Now`, `Rand`等函数。
  - Leader在生成复制日志时，可以通过将不确定性的函数替换为确定值发送给follower，避免数据的不一致。
- 如果语句的执行是依赖于数据库自增字段，或者是其他的数据（`Update ... Where condition`）,则数据库的执行顺序必须完全一致，否则可能导致数据的不一致。
- 如果语句的执行有副作用，例如触发器，存储过程，或则自定义函数的执行，那这些函数也需要是有确定性结果的，在不同的副本内执行时，他们获得的结果是一致的。

#### 基于先写日志(write-ahead log，WAL)

部分存储引擎在执行数据写入中，会将数据写入附加日志。

- 在日志树结构的存储引擎中，日志起到数据存储的作用，所有的数据(日志)都会顺序附加到日志文件中。日志文件会在后台进行压缩
- 基于B-Tree的存储引擎，数据的写入是修改一个磁盘存储块。存储引擎在写入Write-ahead日志之后，才会响应给客户端，修改执行成功，并且将数据写入磁盘。在数据写入磁盘过程中，可能发生断电，导致写入失败。数据库在重启过程中，会使用Write-Ahead Log修复数据库的数据。

这些日志中包含了所有的数据改动，follower通过执行这些日志，既能保持与leader数据的一致性。Postgres，Oracle使用这个方式实现他们的复制日志。这种方式实现的日志是通过描述磁盘块的变化反馈数据的变化，这就要求leader与follower的存储引擎在数据存储到磁盘的格式上是一致的。MySQL的存储引擎是可插拔式的，他的leader和follower的存储引擎可以是不一致的，因而类似的WAL是不适应于它作为复制日志。

在数据版本升级过程，可以通过先升级follower，再通过failover进行切换，再升级切换为follower的老的leader，如果过程中由于升级过程中WAL不能兼容行新老版本，则数据库需要一个下线升级。

#### 基于行的复制日志：Logical (row-based) log replication

逻辑日志（logical log）能够实现复制日志与存储引擎存储数据的格式解耦。

- 数据插入：逻辑日志中包含了各个列的数据
- 数据更新：逻辑日志包含了更新列的数据，以及具体需要更新的数据的唯一标识（主键）
- 数据删除：逻辑日志包含了需要删除的数据的一个唯一标识

一个事故如果修改了多条数据，则会生成多条复制日志，每一条日志标识具体的数据的修改，最后使用一条日志标识以上的记录已被提交。MySQL的binlog使用类似的策略。

基于行的复制日志的优点：

- 与存储引擎解耦，不同的存储引擎，甚而数据库都能使用逻辑日志进行复制。
- 通过逻辑日志可以实现数据仓库数据与数据库的数据同步更新

#### 基于触发器的日志：Trigger-based replication

以上的日志实现都是在数据库内部实现的复制。通过触发器和存储过程也可以实现复制。通过触发器，可以实现在数据更改时，将数据更改同步到其他表，或是数据库中。Oracle的Databus和Postgres的Bucardo就是按照类似的原理工作

## 2. Problems with Replication

基于leader-follower的复制模式，通过增加follower的数量，能够提高读取的吞吐。leader-follower的模式中，只有leader才能写入，如果是同步复制的方式，有一个follower宕机或是网络延迟，则会提高写入的延迟，影响数据写入的吞吐，因而在通过增加follower提高读吞吐的架构中，一般会使用异步复制。

异步复制场景下，follower的数据是通过执行leader的复制日志，能够达到**最终一致性**。日志通过网络传输给follower，因而follower与leader之间的数据存在延迟，如果客户端端直接读取follower上的数据，则可能读取到在leader上已经被更新，当时follower上没有完成的数据。数据复制的延迟在负载比较高，或则网络延迟较高的情况下会更加明显。本节的余下部分，将介绍三个常见的由于复制延迟导致的问题，以及如何处理类似的问题。

### 2.1 Reading Your Own Writes

考虑以下这张图：

![image-20180927000742175](http://liang2020.oss-cn-hangzhou.aliyuncs.com/uPic/blog/image-20180927000742175.png)



以上的场景中，系统使用了多follower的异步复制架构扩展系统的读取性能，客户端连接到leader完成了数据写入，后续客户端读取数据时，连接使用follower2作为读取的数据源，不过由于异步复制的数据延迟，导致客户端没有读取到他之前写入的数据。为了避免以上的场景出现，我们需要**读写一致性**(*read-write-consistency / read-your-write-consistency*)。

对于类似的场景，有许多方法可以实现读写一致性：

- 当用户读取他自己可能修改的数据时，从leader读取，否则则从follower读取。例如当用户读取自己的社交网络的个人信息时，从leader读取，而当读取他人的个人信息是，则从follower读取。如果绝大多数信息对于用户/客户端而言都是可修改的，则这个方案方案最终会把读写压力集中到leader，不能充分得利用follower提高读取吞吐。
- 另一种方案是，客户端/用户如果写入了数据，则在一定时间内，n秒内，从leader进行数据读取，从follower进行数据读取，并且在选取的follower的复制延迟不能高于n秒。
- 客户端记录最后写入数据时间`last_update_time`，选择复制延迟不低于`last_update_time`的follower
- 如果是多数据中心的架构，则写入到leader的请求需要被分发到leader所在的数据中心。



如果用户通过多客户端登录，则需要考虑以下：



- 用户最后更新数据的`last_update_time`不能通过客户端保存，需要保存在数据中心中，才能实现多客户端读取到最新的数据
- 多客户端的请求（PC端通过有线网络，移动端通过4G网络）需要被路由同一数据中心的leader。



### 2.2 Monotonic Read

客户端可能由于复制延迟，读取到过期数据，参考下图：

![image-20181007102940953](http://liang2020.oss-cn-hangzhou.aliyuncs.com/uPic/blog/image-20181007102940953.png)



由于user2345的两次读取选取了不同的follower，不同follower的数据延迟，导致了两次读取获得的数据是不一致的。`monotonic read`能够防止类似异常问题的出现，对于类似的场景，可以通过客户端的hash值选取一个follower，保证读取结果的一致性。

### 2.3 Consist Prefix Read

考虑以下由于复制延迟导致的异常， 由于虽然两个数据是有因果关系，但是由于分布在不同的分片中，其他observer可能由于从不同的replica读取数据导致数据的不一致，类型的场景下，我们需要的是**consist prefix read**, 在后续的章节我们会涉及如何实现consist prefix read。

![image-20181007104238876](http://liang2020.oss-cn-hangzhou.aliyuncs.com/uPic/blog/image-20181007104238876.png)



### 2.4 Solutions for Replication Lag

复制延迟会带来应用层看到数据不一致，在之前的章节中，我们提到了许多方法避免数据延迟对应用层的影响，不过这些代码会增加应用层的复杂度。如果数据库能够帮忙完成这一工作，则能简化应用层的代码。**数据库事务**出现的目的之一也是为了解决类似的问题。

许多数据库支持单机事务，不过由于性能和可用性的代价，在向分布式数据库演化过程中，许多数据库摒弃了对于事务的支持。在后续章节中，会对数据库做具体讲解。

## 3. Multi-Leader Replication

multi-leader，指的是在一个数据存储系统中，存在多个leader，我们可以向这些leader进行数据写入，而这些写入会扩散到所有的数据节点。

### 3.1 multi-leader的应用案例

#### 3.1.1 多数据中心架构

- 多个数据中心中存在多个leader，客户端的写入可以被路由到最近的数据中心，从而提高了数据的写入速度。

- 性能：多个leader可以提高写入并发，使写入的速度不再局限在单个节点的性能。
- 提高数据中心可用性：避免由于leader所在数据中心的断电，断网，或则网络线路异常导致的系统可用性降低或是不可用。

许多数据库支持多leader架构，不过大多需要通过一些工具支持，例如 Tungsten Replicator for MySQL, BDR
for PostgreSQL以及GoldenGate for Oracle。

多leader的架构有许多优点，同时也有许多不足之处：例如多个leader同时修改同一行数据，我们需要解决数据写入冲突。同时数据库的许多特性，例如自增键值，唯一索引约束，触发器等也不同于单个leader内的配置。除非是一定需要，并且能够hold住，multi-leader通常不是一个好的选择。



#### 3.1.2 支持客户端离线操作

试想，我们在一台离线的手机上操作日历上的日程安排，当我们的手机重新上线后，这些数据既能与我们的PC，或是平板上的设备进行同步。这个过程中，每一个设备可以被认为是一个“数据中心leader”。CouchDB通过简单的配置即可实现类似的机制。



#### 3.1.3 协作编辑

实时协作编辑能够让多个用户同时编辑一个文件，Google Docs，Microsoft  OneNote都支持多用户编辑。类似的场景下，每一个用户编辑他所看到的本地副本，然后这个副本被异步得复制到服务器，以及其他查看、编辑同个文件的用户。

如果以上的案例中，每一个用户需要再编辑前获得一个锁，而其他用户不能拿到锁无法编辑，则这个系统则是一个单leader系统，只有一个写入的节点。如果有多个写入的节点，则协作编辑就会遇到类似multi-leader一样的数据冲突问题。



### 3.2 处理写入冲突

Multi-leader遇到的最大问题是数据写入冲突，例如下图

![image-20181021211202144](http://liang2020.oss-cn-hangzhou.aliyuncs.com/uPic/blog/image-20181021211202144.png)



User1和User2同时修改了id=123的数据，如果复制是异步的，数据在其他节点上的修改是延迟的。当复制日志同步到其他节点时，就导致了数据冲突。在单leader架构下，则不会出现写入冲突。



#### 3.2.1 同步与异步冲突检测

以上案例中由于异步复制导致了数据在不同节点上不同，导致了数据冲突。同步复制可以避免类似的问题，在写入数据时，需要确保数据到达子节点后，才确认写入完成。不过这样的方式就丢失了大部分multi-leader带来的优势，不如直接选择单leader的架构。



#### 3.2.2 避免冲突

通过在应用层控制，将可能冲突的数据写入到同一个节点，从而避免数据写入冲突。这也是比较推荐的一种解决冲突的方式。例如，可以将同一个用户的数据路由到同一个数据中心的同一个leader节点，例如按照用户所在的地理位置，将用户分配到最近的数据中心。

在需要修改数据对应的leader时，例如由于系统宕机需要切换leader，或则由于用户的地理位置发生变化导致写入数据的leader变化，还是有可能出现写入冲突，系统还是需要处理冲突。



#### 3.2.3 向一致性状态收敛

复制的目的保证多个replica副本数据一致，如果出现数据冲突，导致不同的节点的数据不一致，这是难以接受的。在多leader架构中，我们需要收敛不同的写入数据，达到数据一致性。

- 通过一个唯一ID（Timstamp， UUID，key value hash值）标识一个数据写入，当出现数据冲突时，选择“最大”的写入作为最终写入的数据。显然，这会导致数据写入丢失，我们在后续的LWW（Last Write Win）的讨论中会涉及这块细节。
- 将不同的leader标识，当出现写入冲突时，选择标识最大的leader来源的写入数据作为最终数据，显然，这会导致数据丢失。
- 通过将冲突数据已字符串的方式汇集，避免数据丢失。
- 已某种数据结构存储冲突，并提示用户解决冲突。OneNote是采用类似的方式解决冲突，实际上，git的代码冲突解决也是类似。



#### 自动处理冲突的方案

最为合适的冲突解决方案需要依赖于应用层处理，许多multi-leader复制工具提供了利用应用层代码解决冲突的解决方案。

- 写触发：
  - 但发生数据写入冲突时，运行处理代码解决冲突。Bucardo (Postgresql的复制工具)能够使用开发者编写的代码处理写入冲突，当然这个过程在后台发生，数据库使用者在写入时无感知。

- 读触发
  - 数据库会记录写入冲突的数据，当应用层读取对应的数据时，数据库会返回冲突的数据，应用层代码需要完成冲突处理，之后数据库会将最终数据写入数据库。CouchDB使用类似的方式。

类似的冲突解决方案通常是针对某一行或是某一个文档的数据，因而当一次写入出现大量的数据冲突，往往也意味我们要解决的冲突不是一个，而是根据发生冲突的数据记录数目而定。

自动冲突解决方案不是万能药，它非常容易变得非常复杂，并且极易出错。 Amazon曾今出现过类似的问题，由于自动冲突解决的代码错误，导致用户的购物车内的商品自增不减。



#### 冲突的定义

以上的案例中，由于不同的leader写入同一个数据导致了一个写入冲突，这是显而易见的冲突。

除此之外，还有其他类型的冲突，例如，如果一间教室在同一时间之允许被某一条记录预定，但是由于不同用户在同一时间操作，在两个leader上都产生了一条订阅记录，这最终导致了冲突。





### 3.3 Multi-Leader的复制拓扑

复制拓扑，指的是数据写入是如何扩散到其他节点的。如果只有两个leader，写入的拓扑是比较简单的双向复制，不过当节点数增加后，复制拓扑可以是多样的。

![image-20181021224457876](http://liang2020.oss-cn-hangzhou.aliyuncs.com/uPic/blog/image-20181021224457876.png)





all-to-all复制拓扑是比较常见的。不过其他的拓扑也有被使用到，例如，MySQL使用Circular拓扑完成复制，每一个节点会接收另一节点的数据，并将它进行转发。start拓扑类似树形结构，一个根节点负责树的转发。

在circular拓扑与star拓扑，节点将他们接收到的数据转发给其他节点，为了避免无穷的数据复制循环，每一个写入需要标识具体写入的节点。当节点接收到自己生产的复制日志时，则忽略对于的日志。

circular和start拓扑节点在某一个节点出现异常后，可能导致复制中断，需要重新调整复制拓扑节点，才能恢复正常的复制。类似all-to-all的连接密集型拓扑，由于节点直接的复制路径较多，对于节点宕机的容忍性更高。

all-to-all复制拓扑同样存在问题，考虑下图：

![image-20181021225756073](http://liang2020.oss-cn-hangzhou.aliyuncs.com/uPic/blog/image-20181021225756073.png)

ClientB的数据更新比ClientA的数据写入更早到达Leader2，Update的数据并不存在，导致数据复制异常。这个同样是由于复制日志之间存在因果关系，当时复制日志的执行并不遵循因果关系导致的冲突。通过时间撮确定日志的顺序并不能完全避免类似的冲突，因为不同机器上的时间不能保证是一致的。

版本向量version vector的技术能够帮助我们保证这些事件的顺序，在后续的讨论中我们会提到这块技术。不过冲突检测在许多复制系统中还是实现得很简单的。例如，PostgreSQL的BDR并不能确保数据写入的顺序，Tungsten Replicator for MySQL甚而不会检测写入冲突。

在选择multi-leader复制系统时，非常有必要仔细研读文档，并充分测试，以确保系统的可用性，所能提供的数据一致性是能够满足需求的。



## 4. Leaderless Replication



leaderless复制系统抛弃了leader和follower的概念。在复制系统的最早期都是leaderless架构，不过随着关系型数据库的崛起，它逐渐被遗忘了。在Amazon开发上线了DynamoSystem后，leaderless系统又回到了大众的视野，Riak, Cassandra, 和Voldemort等这些leaderless系统，都是受到了Dynamo启发，因而这类存储架构又被称为Dynamo-style。

部分leaderless复制系统实现中，client直接向不同的节点写入数据，还有其他一些leader复制系统，则会使用一个coordinator node协调节点，协助客户端完成数据写入。与leader节点不同，协调节点不会保证数据写入的顺序



### 4.1 当某一节点宕机时

![image-20181021233541129](http://liang2020.oss-cn-hangzhou.aliyuncs.com/uPic/blog/image-20181021233541129.png)



在数据写入发生时，出现了节点宕机，例如上图的Replica 3。客户端将数据写入同时发送给了3个节点，1，2节点返回写入成功，3节点写入失败了。在这种情况下，我们依然认为数据写入是成功的。

如果后续节点又恢复到线上时，我们尝试读取数据，会读取到离线节点上的脏数据。为了解决类似的问题，客户端在读取时，会尝试向多个节点发起请求，返回的数据可能有过期的脏数据，也有正确的数据，客户端通过返回数据的版本号判断应该选取哪一个数据。



#### 4.1.1 读修复和反熵 Read repair and anti-entropy

复制系统需要确保不同节点内的数据达到一致，在leadeless系统内如何处理宕机后又恢复上线的节点呢，如何让节点赶上目前的进度呢？在Dynamo系统中有两种机制完成这个工作：

- 读修复：
  - 客户端读取时会尝试向多个节点发送请求，当发现有过期数据后，会向过期节点写入客户端读取到的最新版本的数据
- 反熵处理
  - 系统的后台，有反熵程序持续不断得比较节点数据之间的不同，并同步这些数据，达到数据一致性。同样，这个机制并不能保证数据写入顺序，并且有较高的数据延迟。

并不是所有的leaderless系统都实现了以上的机制，Voldemort并没有反熵机制，部分读取频率低的数据，可能由于部分节点宕机而永远丢失。



#### 4.1.2 读写共识 Quorums for reading and writing

在之前的案例中，我们在3个节点中成功写入了2个节点，读取时，至少需要读取2个节点的数据才能确认读取到最新的数据， 2 + 2 > 3。实际的原则是，我们至少能够读取到一个节点内的最新的数据写入，如果推广到一个有n个节点的系统，为了达成共识，需要满足，w + r > n。其中w标识成功的写入至少要写入的节点数，r表示至少需要读取多少个节点的数据，符合要求的读写请求被称为多数读/多数写(quorums write/quorums read)。

在Dynamo System中，w与r通常是可配置的。一种常见的配置方式是，n为奇数，设置w=r=(n+1)/2；这个并不是固定的，可以根据实际需求调整，例如，在一个写负载比较低，读负载比较高的场景下，我们可以w设置为n，r设置为1，这样能够大大提供读的效率，不过如果一个节点宕机，则不能完成写入，导致系统宕机。

合理配置的w，r能够让系统对于部分容忍部分节点宕机：

- w < n：能够在部分节点宕机后进行数据写入
- r < n: 支持节点宕机后数据读取
- n=3, w=2, r=2, 在最多一个节点宕机后能够正常读写。
- n=5, w=3, r=3，在最多两个节点宕机后能够正常读写（见下图）。

![image-20181104221015569](http://liang2020.oss-cn-hangzhou.aliyuncs.com/uPic/blog/image-20181104221015569.png)

### 4.3 共识一致性的缺陷

将w和r配置为大多数(> n/2)，系统最多能够容忍小于n/2的节点的宕机，实现系统的读写一致性。不过在一些场景下，这个一致性是不能保障的。

- 使用sloop quorum后，不能确保w与r一定会有重叠（后续章节中会有涉及）
- 出现并发写入冲突时，无法确认写入的先后顺序。如果使用last write win，可能由于不同服务器的时间不一致导致数据写入丢失
- 出现读写并发，写入可能只在部分节点成功，无法确认读取获取的数据是新的数据还是老的。
- 当写入失败时，部分节点写入成功，但写入节点少于w，无法确定读取获得的数据是失败的数据，还是老的数据。
- 一个节点出现单机，使用一个旧的副本备份重启恢复节点，则可能出现在原先节点上写入的数据由于不能满足多数写，造成数据丢失。
- 即使系统工作正常，依然可能由于时间问题出现异常，在后续“线性一致性与共识”的章节中我们会涉及这部分的讨论

Dynamo System看似是能够确保应用能够读取到最后写入成功的数据，但实际场景中它不能保证。它是为能够容忍**最终一致性**的应用而设计的。如果



#### 4.3.1 过期数据监控

Leader-Based的系统中，follower线性执行leader的复制日志，通过复制日志的执行进度，可以明了得知道复制系统是否存在延迟。leader-less系统的写入并不是有序的，并且可能存在读修复，如何判断系统的延迟是比较困难的。

已有一些研究尝试检测leader-less系统的数据延迟情况，以及数据读取可能出现延迟的比率[1]，不过目前并没有一个通用的解决方案。

### 4.2 Sloppy Quorums

当出现网络异常时，客户端可能不能连接到所有的节点。客户端能够进行读写的节点会小于w和r。客户端面临抉择

- 如果服务节点不能满足w和r的需求，直接返回错误
- 接收写入请求，将输入写入到在n之外的备用节点中

第二种方式就是sloppy quorums，系统仍然要求的读写满足w+r>n，不过这里的包括了n之外的部分备用节点。当网络恢复后，备用节点的数据会写回到n个节点内宕机的那些节点。

sloppy quorums增强了系统写入的可用性，不过也面临可能导致 即使 满足 w+r > n，也有可能读取的数据也不是最新的数据，可能是之前断线，刚刚恢复的节点。



#### 多数据中心操作

leaderless系统是为容忍并发写，网络中断，延迟而设计的，因而也适合于多数据中心部署。

Cassandra，Voldemort可以配置支持多数据中心。n个节点分配到多个数据中心中，客户端写入时，只会同步等待同个数据中心的节点的写入结果，其他的数据中心的写入可以异步（也可以同步）完成，从而避免了由于网络延迟和中断对于数据写入的影响。

Rika只会与同个数据中心内的节点进行读写，不同数据中心之间的数据的同步是异步完成，他的实现方式类似multi-leader 系统。



### 4.3 写入冲突

Quorums系统允许并发写入，这也导致了可能出现写入冲突。



![image-20181104234121200](http://liang2020.oss-cn-hangzhou.aliyuncs.com/uPic/blog/image-20181104234121200.png)

以上的案例中，由于写入的请求可能以不同的顺序到达不同节点，导致了数据不一致。node 2节点认为最新的数据是B，其他节点和客户端认为最新的数据是A。

当出现以上并发写入时，我们需要对写入请求进行仲裁：

#### 4.3.1 Last Write Win

为每一个写入带上一个时间戳，服务端根据时间戳确认写入顺序，使用最后写入的请求，并且抛弃任何早于最新时间的值。如果多个写入同时进行，虽然给客户端的结果响应是写入成功，但是仍然可能由于同时的并发写入而被覆盖了。同时，不同的服务器上的时间戳可能纯在同步问题，导致时间戳不能反馈真正的先后顺序。

- 使用 LWW 安全的前提是确保一个 key 只被写入一次然后被当做不可变，从而避免了对同一个 key 的并发写入，这也是Cassandra的推荐做法。



#### 4.3.2 “happens-before”关系

并发问题并不完全是时间问题，两个操作没有相互感知，无论时间是否一致，都可以认为是并发。

我们需要有一个算法，确认两个操作，A与B的关系，他们可能是A before B，可能是B before A，或则他们是并发的。

如果A与B有先后关系，我们需要使用后写入的数据进行覆盖，如果数据没有先后关系，我们需要解决冲突。

如何捕获happens-before关系呢：通过维护在服务端维护key的版本号，并且每次写入是

- 服务端维护key的版本号，并且每次写入后，将新的版本号存储入写入的之中

- 客户端端写入一个key之前，需要读取key的值，以及他的版本号，进行写入时，必须带上原先key的版本号，并且合并原先的值。
- 服务端接收到写入请求后，可以覆盖写入的版本号以及以下的所有值，因为他知道这个新的写入是高于当前版本的。







