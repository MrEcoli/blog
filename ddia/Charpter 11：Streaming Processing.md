# Chapter 11 流处理



在上一章节中（Chapter 10 批处理 Batch Processing），我们讨论了批处理技术，它通过读取文件/数据集，生成输出衍生数据。他被广泛得应用于搜索索引构建，推荐系统，以及数据分析等应用场景。

对于批处理技术的一个假设是：输入数据集的大小是固定的。例如需要对一批数据进行排序，MapReduce程序需要将所有的数据集读取完之后，才可以开始输出结果集。

大多数业务场景中，数据是持续不断产生的，用户随时随地都可能访问服务，发生各类事件，输出各种访问日志等数据。除非服务宕机或是关闭服务，数据流是不会间断的。

按日间隔的批处理会有比较高的延迟，无法满足对于延迟要求比较高的应用场景。可以通过缩短批处理运行的间隔，提高程序运行的频率，降低批处理的延迟，例如将批处理的运行间隔降低从每日降低到每小时，每分钟，甚而每秒。换种思路，如果我们在数据产生时，即对数据进行处理，则可以大大降低事件的延迟，这即是**流处理**的思想。

在本章节中，我们会讨论**流处理**技术：首先，我们探讨流是如何表示，存储以及在网络中进行传输；在**数据库与流**这一节中，我们会探讨数据库与流的关系；最后，在**Processing Stream**这一小节中，我们会探索处理流的手段与方法，以及如何将这些技术应用于构建我们的应用。

##流的传输（Transmitting Event Streams）

流处理的上下文中，处理的记录被认为是一个**事件**，通常有一个时间戳的属性，表示事件发生的时间点，并包含一系列的其他属性，数据，记录这个事件的具体内容。例如用户的一次购买行为，或则是一次商品浏览，或则是监控系统对于CPU利用率的一次度量，或则是web服务器的每一次的访问日志，都可以成为流中的一个事件。

事件可以存储为文本，JSON或则其他的二进制格式，能够写入关系型数据库，或则其他的NoSQL，文件系统，并且通过网络进行传输，使得事件的生产和消费可以在不同的节点上进行。

在流处理术语中，流中的事件，由一个生产者（producer，publisher，sender）产生，可能被多个消费者消费（consumer，subscriber or recipients）。在文件系统中，多条记录被汇集在一个文件中，通过具体的文件名标识。在流处理中，事件通过topic进行聚合。

理论上，我们可以通过文件系统，或则数据库，连接生产者和消费者。生产者往数据库或则文件系统中写入数据，消费者定期拉取文件系统和数据库中的新事件。这本质上与批处理的流程是一致的，我们定期拉取数据，完成新增的数据处理。

依赖于定期拉取新数据的模式，不适用于对于延迟要求较高的系统。提高拉取的频率，每次拉取到新的数据的概率也会降低，同时会提高系统的负载。在新的事件到来后，推送提醒消费者则能有效得减低延迟，减少拉取行为带来的负载。

数据库对于此类的推送的支持并不完美：虽然数据库有触发器（trigger）机制，但是他所能做的事情非常有限。有许多专为此类推送机制而设计的工具能够替代数据库在流处理中的作用。

### 消息系统（Messaging System）

消息系统是实现将事件推送给消费者的一种方法: 生产者产生数据，并将数据推送给消费者。

UNIX的管道和TCP连接是常用的消息系统中生产者和消费者的通讯方式。消息系统会以此做扩展，Unix管道与TCP连接只能支持一个生产者和一个消费者，而消息系统允许多个生产者向一个topic推送消息，同时也允许多个消费者向同一个topic订阅消息。

为了实现发布者/订阅者的模式，不同的系统使用了不同的方法。以下两个问题，能够有效得区分这些系统。

1. 当生产者生成消息的速度高于消费者的消费能力，系统是如何处理的？

   简单得说，有三种方案：1) 将消息进行暂存，2) 或者将超出消费能力的消息丢弃，3) 或者使用backpressure进行控流，阻断生产者进行消息推送。Unix pipe与TCP都是使用backpressure进行控流，他们底层都有一个固定大小的buffer存储消息，当buffer已满的情况下，向buffer中推送消息会被block, 直到buffer中有空闲空间。

   如果使用队列进行buffer，我们需要考虑队列不停增大的场景，他可能会超过内存大小。是否会持久化到磁盘呢，磁盘的性能是否会影响消息的写入。

2. 当节点宕机或是是临时离线的情况下，系统是如何处理的，是否会有消息丢失？

   数据库使用先写日志，以及复制保证数据的可用性，这些技术也带来了性能损失。如果能够容忍少量的数据丢失，系统则可以获得更高的吞吐以及更低的延迟。

是否能够容忍消息丢失取决于应用层的需求。对于监控系统，丢失的一条硬件指标数据并不会造成太大的影响，后续的消息会更新对应的数值，不过如果大量的数据丢失，则我们失去某一段时间内对于系统的监控。如果应用层依赖于消息进行计数，则丢失的消息则造成计数错误。

#### 生产者直接投递消息给消费者

许多系统不依赖于一个中间节点进行消息的存储，转发 or推送。

- UDP广播被广泛得用于金融市场中，例如股票市场的feeds流中，他们需要低延迟的消息广播。虽然UDP在协议上是不可靠的，不过可以通过应用层的重发机制保证消息的可靠性。
- Brokerless消息队列ZeroMQ，nanomsg直接使用TCP实现发布者/订阅者模型。
- StatsD和Brubeck使用不可靠的UDP传输收集的系统监控信息。
- 如果消费者能够通过RPC或则HTTP等方式接收消息，则生产者也可以将消息直接推送给消费者。这其实也是webhook的思想，当事件发生时，将事件通过HTTP请求发送给webhook的接收方。

以上的这些系统能够很好得满足一些应用得需求。不过值得注意，他们的可能导致消息的丢失，即使存在消息重传机制。系统的稳定运行的前提是消费者始终在线， 当消费者宕机或则离线，生产者会使用消息buffer进行重传，不过这些buffer可能由于生产者的宕机而丢失。

#### 消息队列

大多数消息系统使用一个中间件，**消息队列**，进行消息的中转。他是一个数据库的变种，专为处理消息流而优化。它已一个服务的形式运行，生产者和消费者以客户端的形式连接到消息队列，生产者向其推送消息，消息队列向消费者推送消息。

使用消息队列，将数据集中集中存储，能够容忍客户端的宕机，短连，重连，系统的可用性压力转移到了消息队列本身。部分消息队列将数据存储在内存，部分能够（通常是可配置的）将数据写入到磁盘，能够避免消息队列服务宕机导致的数据丢失。如果由于消费者消费过慢，造成消息堆积，通过磁盘也能够极大得增强队列存储的数据集大小。

使用消息队列的一个结果是，事件的处理是异步的。生产者将消息投递给消息队列，等待消息队列的确认，但是不知道消息是在什么时候被消费处理的。消息的消费通常很快发生，不过当队列发生拥塞时，则延迟会大大增加。

#### 消息队列与数据库的区别

部分消息队列支持两阶段提交完成消息的投递和消费，这个特性使得他们与数据库十分相似。不过，消息队列在许多方面与数据库存在较大差异。

- 数据库中的数据需要明确的删除指令才会被删除，而消息队列中的数据在被成功投递给消费者之后，就会被删除。
- 数据库支持二级索引或者是其他的消息搜索方式，消息队列则是支持订阅指定topic的消息。虽然二者的机制存在区别，实际上都是提供客户端某一个数据查询机制。
- 查询数据库时，数据库提供给客户端当前时间点的数据快照，随后的数据写入并不会通知客户端他的数据查询已经outdated了。在消息队列系统中，数据的写入会推送给客户端。



#### 多消费者

当多个消费者同时订阅一个topic，主要有以下两种方式进行消息的投递。

- 负载均衡

每一条消息，会被确切投递给某一个消费者，不同的consumer可以并行得处理不同的任务。此类的投递模式适用于消息处理是成本较高（每条消息处理成本 * 消息处理数量=处理成本），期望能够通过增加cosumer提高并行处理消息的能力。

- 发散模式

每一条消息会被投递给所有的消费者，使得相互独立的consumer能够同时处理事件，确保事件能够准确，快速得被处理。

![image-20181125192726372](/Users/liangquan/Library/Application Support/typora-user-images/image-20181125192726372.png)

两种模式是可以结合的，例如对consumer进行分组，分组都会订阅同一个topic。消息会投递给所有的分组，不过每一个分组中只有一个节点会受到消息。

#### 传输确认与消息重传

consumer可能在任何时间点宕机，消息被投递给consumer之后，consumer可能在接收到消息后宕机，或则是在消费任务被执行到一半的时候宕机。消息系统使用**确认机制**保证消息的执行，当consumer完成消息的消费流程后，会发送一个确认信号给消息队列，消息队列才会删除对应的消息。

如果consumer与消息队列的连接被断开，或则 消息队列队列等待consumer的结果超时，则消息队列认为consumer没有执行对应的消息或则执行失败，会将消息投递给另外一个consumer（实际上，消费者可能成功执行任务了，当时确认消息在网络传输中丢失了，对于此类case，依赖于具有原子性的commit方法）。

消息重传与负载均衡结合，会导致一些意外的结果。考虑下图（Figure 11-2），m3在m4之前产生，但是由于第一次处理失败，进行了消息重传，导致m3实际处理的时间点是在m4。消息生成的时间与处理的时间点是不一致的。



![image-20181125195331905](/Users/liangquan/Library/Application Support/typora-user-images/image-20181125195331905.png)

即使消息队列尝试保证消息处理的顺序性，负载均衡与消息重传机制结合必然导致事件处理顺序重排序。为了避免此类问题，可以考虑每一个队列使用一个consumer（牺牲了负载均衡机制），保证消息的顺序性。

消息处理是否顺序，并不会影响各自独立的消息事件的处理。不过如果消息之间有严格的因果依赖，则消息系统需要保证消息的严格顺序，在后续的章节中我们会讨论这个问题。









———————— 第二次递交 分割线———————— 

———————— 第二次递交 分割线———————— 

———————— 第二次递交 分割线———————— 











### 分区日志

通过网络传输传递任务，对于任务数据而言并不是持久的。即便我们可以通过抓包工具获取数据，记录日志，不过这并不是一个常见的做法。即便消息系统会将接收到的数据存储到磁盘，在完成消息投递或则收到任务完成后，便会将对应的消息删除，他们大多也是基于消息并不是持久化存储的思想设计的。

日志系统与数据库与以上的思想恰恰相反，每当数据写入后，便会持久化存储，直到数据被删除。

这个思路的不同影响了各类派生数据是如何被创建的。在Chapter10，批处理系统中，处理的流程不会影响原始的输入数据，我们可以多次对原先的数据调用批处理程序，生成衍生数据。而在AMQP/JMS类型的系统中，消息会在consumer发来ack，确认消息的处理后，删除消息，consumer无法重复得消费同一条消息。

如果我们给消息系统增加一个client，这个client也只能消费他加入的时间点后，还未被消费得消息，无法得知之前的消息。而在文件系统或是数据库系统中，之前的消息只要没有被删除，新加入的客户端依旧能够读取到。

基于**日志的消息系统（log-based message brokers）**的设计初衷，就是为了将数据库系统的持久化存储特性，以及消息系统的低延迟消息推送进行结合。



#### 使用日志存储消息

日志是在磁盘上存储的，连续得附加的记录。在此前的章节中，日志树存储引擎，先写日志，以及复制中，均与日志相关。

基于日志实现的消息系统的基本思路是：生产者产生消息，附加写日志文件，而消费者通过顺序读取日志，获取消息。当消费者读取到日志尾部，他会等待一个新消息到来的推送。Unix工具，`tail -f`使用类似的策略，他监控文件的修改，输出文件附加数据到标准输出。

为了提供系统的吞吐，存储到单个磁盘的日志可以被分区存储到不同的磁盘。不同的分区日志也可以被分区存储到不同的机器上，每一块分区日志能够进行自己独立的读写操作。一个topic可以将多个分区聚合，他们进行类似消息的传递。参见下图。

![image-20181216213554159](/Users/liangquan/Library/Application Support/typora-user-images/image-20181216213554159.png)

Apache Kafka， Amazon Kinesis Stream，Twiiter的DistributedLog都是基于日志的消息系统。Google Cloud的Pub/Sub架构上也是类似，不过暴露的是类似JMS的API。这些消息系统虽然将所有的消息写入到磁盘，但是利用将日志进行分区到不同的服务器，依然能够达到百万级别每秒的消息的吞吐，并且能够通过日志复制实现高可用性。

#### 日志与传统消息系统对比

基于日志的消息系统非常容易实现fan-out消息广播：多个客户端可以同时读取同一个日志文件，并且在消息处理完成后不会将消息删除。为了能够实现负载均衡，消息系统将消息投递给多个consumer，而基于日志的消息系统则可以将多个分区的消息投递给指定的consumer group。

每一个consumer客户端都可以接受到被指定的分区日志的所有消息。通常一个consumer client被指定消费一个分区日志后，他会以单线程的方式，顺序的读取分区日志。以上的做法有两个缺点：

- 并发处理某一个topic的consumer的数量，取决topic的消息被分区为几个。
- 当某一个分区日志内的消息处理花费过长时，会同时导致后续的消息处理被延后。

因而，当消息处理代价昂贵，并且我们期望能够达到逐消息的并发，并且消息的顺序并不是那么重要，那么JMS/AMQP风格的消息是更好的选择。相反的，如果消息处理的顺序很重要，消息的吞吐要求较高，并且消息处理的速度很快，那么基于日志的消息系统则是更好的选择。



#### 消费者偏移

顺序消费使得区分消息是否已经被消费处理变得非常容易：所有在当前处理的消息偏移之前的消息都是已被消费的，在此偏移之后的消息都是未被消费的。所以消息系统也就不需要追踪每一条消息的acknowledge，它只需要定期consumer消费的消息的偏移。这个方案也大大降低了消息系统的负载，提高了消息系统的吞吐。

consumer的消息消费的偏移类似之前章节Chapter5复制 中讨论的复制日志的偏移。在数据库复制系统中，日志顺序偏移的存在，使得slave在于master与断开连接重连之后，能够重新开始复制，并且不会丢失复制数据。在log-base消息系统中，日志偏移也承担相似的作用，这个过程中，consumer承担了slave的角色。

当一个consumer节点宕机，另外一个consumer group中的consumer节点被赋予继续消费宕机节点的分区日志，他可以从之前宕机节点消费的最后一条记录的偏移开始。如果consumer已经消费了部分数据，但是最后偏移并未更新，这将导致部分消息会被消费两次。我们在后续的章节中将会讨论这个问题。

#### 磁盘空间的使用

如果我们只是不停得附加写入磁盘，磁盘的空间始终会被耗尽。为了重复利用磁盘空间，日志被分割为不同的片段，老的片段会被删除，或则被移动到归档存储系统中。

如果一个慢的consumer不能跟上消息生成的速度，落后于需要删除的片段的偏移位置，他会导致部分消息的丢失。事实上，日志大多数是已固定大小的文件进行轮转，当文件大小达到阈值，则会删除部分老的数据，此类方式又称为circular buffer或则是ring buffer。不过由于数据是buffer到磁盘，它的容量可以非常大。

一个大机械磁盘可以有6TB容量，顺序写入的能力大概为150MB/s。以最高的写入速度，写满这个磁盘需要11小时，然后才会开始丢弃老的消息。通常数据的写入不会将磁盘带宽打满，通常这个过程需要数天甚而数周。

不论我们保存多久的数据，每一条数据的吞吐成本是一致的，因为每一条数据都会被写入到磁盘中。这与将消息存储到内存中的消息系统不同，他只会在消息对接过大后将数据写入磁盘，并且在此之后写入速度下降，因而他们的吞吐取决于历史数据的数量。



#### 当消费者跟不上生产者

在之前的**消息系统**的小节中，我们讨论了如果consumer的消费速度没能跟上生产者生成消息的速度时的三种处理方案：丢弃消息，暂存消息，或则控流消息生成。按照这种分类方式，基于log的消息系统使用消息暂存机制，将消息**暂存**到磁盘文件中。

如果consumer落后得太远，可能导致部分消息已经被删除了，但是他依旧未能消费到。系统可以通过监控消费者落后得进度进行报警，由于磁盘作为buffer的容量较大，出现这种情况时，一般有充足的时间足够运维或是工程师进行排查或是调整。

即便一个consumer由于落到太远以至于出现消息丢失，也只会影响到对应的消费者。他不会打断其他的消费者。这有一个好处：我们可以开启一个consumer消费生产日志，用于调试，或是测试，或是其他目的，而且不同担心它会影响到线上生产环境的consumer。当我们完成之后，可以直接关闭consumer，唯一遗留的是这个consumer的消费偏移。

这个行为模式与经典的消息系统不同，我们需要小心得删除任何消费者已经被关闭的队列，否则他们会持续积累，并且占用掉大量的内存。

#### 重放老的消息

在AMQP或是JMS形式的消息系统中，消息处理以及确认消息是一个不可逆的操作，它会导致消息从系统中删除。在日志形式消息系统中，消费消息是一个只读行为，他更像是读取文件，并不会导致log的修改。

日志消息系统的消费消息唯一的副作用是，consumer对应的消费偏移会修改。不过这个偏移的修改是有consumer控制的，我们可以非常容易的实现一个consumer，他的消费偏移是从昨天开始，生成的数据目的地与之前的consumer不同，用于重新处理昨天的数据。

从这个角度看，log-based消息处理更像是之前提到的批处理，派生数据与输入数据完全分开，并且可以进行重复处理和转换。他方便了更多的实验探索，并且能够更加容易得从错误或是bug中恢复，使得他成为数据流处理中的一个优秀的工具。



———————— 第三次递交 分割线———————— 

———————— 第三次递交 分割线———————— 

———————— 第三次递交 分割线———————— 





## 数据库与流

以上的讨论中，我们讨论了数据库与流的区别，流使用许多数据库中理念。本节中，我们会看看流处理技术在数据库中应用。

事件是一条发生在某个时间的记录，他可能是一个用户行为，也可能是一个传感器的数据读取，或则是对数据库的一次写入。写入数据库的事件，能够被捕获，存储或则进行其他处理。从这个角度看，数据库与流存在更深的联系。

在复制的章节中我们讨论过，数据库系统通过复制日志，实现follower与leader的数据一致。复制日志中的这些事件描述了在数据库中发生的数据改变。

在本章节中，我们会看下异构的数据系统带来的问题，以及如何处理。

### 系统数据同步

在过往的讨论中，我们知道，没有一个存储系统可以满足所有的数据存储，查询或是处理的需求。在大多数应用，我们需要使用多种存储系统满足需求，例如：使用OLTP数据库保存关联数据，使用cache加快请求，使用全文索引处理搜索请求，使用数据仓库进行数据分析。每一个系统都会已他们优化的方式进行数据的存储和查询。

如果相同或是相关的数据，已不同的形式存储在不同的系统中，不同的系统对于数据需要保持同步。例如一个对象在数据库中被更新，那它在缓存中的数据，搜索的索引，以及数据仓库中的数据都需要被更新。通常，数据仓库中的同步是通过ETL使用数据库的一个完成拷贝，进行转换，批量转存到数据仓库，换而言之，是通过批处理完成。在之前的批处理的章节中，我们也提到过，搜索索引，推荐系统，或是其他的衍生数据系统，往往也可以通过类似的批处理机制完成。

如果数据库过大，完整得通过批处理完成异构数据系统导入的成本过高，双写（dual write）也是一个完成数据同步的替代方案，例如：先将数据更改写入数据库，然后更新索引，然后失效对应的数据缓存。

不过双写有许多严重的问题，下图中展示了一个双写导致的数据冲突。Client A对于搜索索引写入的延迟，导致了数据库与搜索索引中的数据不一致。



![image-20190101200753888](/Users/liangquan/Library/Application Support/typora-user-images/image-20190101200753888.png)

如果没有并发写入检测机制，例如之前章节的版本向量，对应的并发写入冲突我们甚而完全无法觉察。

另一个双写可能导致的问题是，可能出现某一个写入成功，另一个写入失败。这是一个对于错误容忍的问题，而不是一个并发写入问题。保证写入同时失败或则同时成功是一个原子性提交问题，他的解决代价昂贵。

在只有一个leader的复制系统中，由leader决定写入顺序，在不同的follower中已相同的顺序进行数据写入和复制。但在上图中（Figure 11-4），Database是一个leader，SearchIndex同样是一个leader，都并非对方的follower，因而出现写入冲突。

### 捕获数据修改 (change data capture CDC)

数据库系统的复制日志都是以一个内部系统方式实现，而不是一个公开的API。连接的客户端，可以通过定义的查询的语言和一定的搜索模型能够进行数据库存储数据对象的查询，但是不能够解析复制日志，并且能够从中提取数据。

在过去数十年中，数据库很少有文档提供能够获取更新数据的日志，因而也很难实现通过日志获取数据变更，从而在异构数据系统中，例如缓存，搜索索引中进行数据的更新，进行数据同步更新。

在近期几年，捕获数据修改(change data capture CDC)正在获得更多关注。他指的是，通过捕获数据修改日志，抽取实际的数据修改，将数据修改复制到其他系统中。CDC可以实时获得数据修改信息，他的本质其实是一条流。

举个例子，我们可以获取数据库的修改，并将这些数据修改在搜索索引中重放。如果修改的顺序与数据库中是一致的，那么我们可以预期，搜索索引中的数据与数据库中是一致的。这个过程中，搜索索引，或则其他衍生的数据系统，都是这个数据修改流的消费者，如同下图所示。

![image-20190101213222736](/Users/liangquan/Library/Application Support/typora-user-images/image-20190101213222736.png)

### 数据修改捕获的实现

存储在搜索索引或是数据仓库中的数据，是数据在系统中的另一个视角。CDC是确保在衍生数据系统中的数据，能够与源数据保持一致。

另一个角度讲，CDC使得数据库成为一个leader，而其他的数据系统则为follower。一个log-based的broker非常适合用于传输源数据库的修改事件，因为他能够确保事件的有序，避免乱序导致的数据不一致。

通过注册监听所有的数据修改，数据库的触发器机制能够用于实现CDC，从而将所有的数据修改写入到一张修改日志的数据表中。然而这个机制是脆弱，并且严重影响数据库性能的。通过解析复制日志获取数据修改，看上去是一个更为健壮方式，并且也不会影响数据库性能，不过却也面临很多其他问题，例如 如何处理数据表Schema的变更。

有许多的系统使用以上的思路实现获取数据库变更同步到其他的衍生系统，例如LinkedIn的Databus，Facebook的Wormhole，Yahoo的Sherpa。Bottled Water使用一个PostgreSQL的API解析复制日志，实现PostgreSQL的CDC。MaxWell与Debezium通过解析binlog在MySQL上实现类似的功能。

与消息系统类似，CDC通常是异步实现的：修改日志记录的写入提交，并不会等待衍生系统的消费。这避免了较慢的消费者拖累了日志的写入，但同时也会到来类似复制日志延迟导致的一系列后果（参见复制相关章节）。



———————— 第四次递交 分割线———————— 

———————— 第四次递交 分割线———————— 

———————— 第四次递交 分割线———————— 







## 	事件溯源 Event Sourcing

事件溯源（Event Sourcing）是领域驱动设计（Domain-Driven Design）的一个架构模式。他以事件作为系统的第一公民，业务由事件驱动完成。

以账户余额为例 https://www.imooc.com/article/40858：

![image-20190120203950071](/Users/liangquan/Library/Application Support/typora-user-images/image-20190120203950071.png)



在这个图中，中间的是我们的账户对象，它有几个handler函数`create()`, `deposit()`, `withdraw()`，分别用于处理新建账户、账户存款和取款的操作。

左边的就是一个个的事件，它是一个事件的流，根据用户请求或者从其他地方产生。在这里例子当中，有3个事件：`AccountCreated`, `AccountDeposited`, `AccountWithdrawed`，分别相当于账户创建的事件，存款的事件和取款的事件。当这些事件产生的时候，我们会触发上面的Account对象的相应的处理函数。

右边的就是这个Account对象处理完左边的4个事件以后，最新的数据状态。具体的处理过程就是：

1. 系统产生一个新建账户的事件`AccountCreated`，`Account`对象来处理这个事件，事件里面的Id是1234，系统先尝试着找id是1234的账户，发现没有，于是新建一个`Account`对象，并在它上面调用`create()`的处理函数，也就是初始化了id和余额。
2. 然后，有一个`AccountDeposited`的事件，对应的`Account`对象的id是1234，系统找到之前创建的对象，在它上面应用`deposit()`处理函数，也就是增加的余额的操作，更新了账户余额。
3. 系统又收到一个存款事件，跟上面一样，又更新了一次余额。
4. 收到一个取款事件，还是找到id是1234的账户，在它上面调用`withdraw()`的处理函数，进行取款操作，更新余额。
5. 最后，1234这个账户的余额是200，也就是右边的数据状态。

事件会被持久化存储，而当前的视图状态依赖于事件计算得到。与CDC类似，事件溯源同样会存储应用得状态变化日志。不过二者在基本思想还是存在较大不同。

- 在CDC中，应用存储在数据库的数据是可变的，记录会被删除或是更新。CDC通过获取数据库中的数据变化日志（例如：解析复制日志）获取数据的变化，并且通过保证数据修改日志的顺序，保证不会出现事件冲突。应用层并不知晓CDC的存在。
- 在事件溯源中，应用层的逻辑建立在由应用层写入的应用日志（event log）上。事件存储（event store）只能附加写，删除或是更新大多数情况是不受支持的。事件是反馈了应用层的逻辑，而不是底层的逻辑变化。

事件溯源在数据建模中是一个非常有用的工具：从应用层的角度，记录用户的行为记录，比单存的一条数据库中的最终结果字段更有意义。事件溯源使得应用的迭代更加简单，通过事件记录，使得debug更为简单，同时能够抵御一定程度的应用层bug。



### 以事件溯源获取当前视图

单独的事件从用户层看，意义不大，因为用户期望是看到是当前系统的状态，而不是历史的改动。举个例子，当用户查看购物车时，期望看到的是当前购物车内的商品，而不是购物车更改的一个或是多个记录。

应用层需要将这些事件日志转换为可以展示给前台用户的应用状态。这个转换的逻辑由应用决定，不过他应该是确定性的，以保证应用层每次使用这些事件日志，获得的应用状态是一致的。

与CDC类型，重放历史的事件日志能够复现当前的应用状态。不过在日志压缩上，二者的处理是不同的。

- CDC日志包括了最新版本的记录的所有数据字段，因而最新版本的日志能够反馈应用的最新状态，日志压缩只需要保留最新的数据，丢弃之前的版本即可。
- Event Sourcing的每一条记录表示了一个行为记录，最新的事件也只是反馈了最后一次行为事件，他不能反馈应用的最终状态，需要依赖于历史记录。因而应用状态需要所有的历史数据才能获得。

使用event sourcing的应用，一般会有机制能够保存应用状态快照，从而使得应用不需要重复所有历史记录。这是一个性能优化，用于提高读取速度以及从宕机中恢复的速度。当需要时，系统还是需要有能够从完整的历史记录中重现应用状态的能力。



### 命令与事件

事件溯源的哲学清晰得区分了*事件（event）*以及*命令（command）*。当一个用户请求发给应用时，这是一个命令，它需要经过校验，例如唯一性检验，确认可执行后，才会生成一个事件，而事件是可以持久化存储，并且不可变的。

例如用户想要注册一个新的用户名，或则预定电影院的一个座位。这个过程中，应用服务需要检查用户输入的名字是否已被使用，预定的座位是否已被其他用户预定，如果出现冲突，则不会执行命令，也不会有事件生成。检验通过之后，应用才会生成事件，某一个用户名已经被一个UserId的用户使用，某一个座位被某一个用户预定。

当事件生成之后，他就变成一个事实（fact）。即使用户后续想要取消预定，并不会改变之前的事件，只会生成新的事件，用户修改座位或则是取消预定的事件。

事件流的消费者是无法拒绝事件的，因为当消费者看到事件之后，事件就是不可变的，并且他可能被其他的消费者看到。因而，所有对于命令的校验需要以同步的方式进行：例如以序列化事务原子性得检验命令，推送事件。

如果将用户预定座位的行为分隔为两个事件，试探性预定事件，如果试探性预定成功则写入座位预定事件（参考 Chapter 9：数据一致性的实现）。通过分隔行为，可以将检验在异步中完成。

